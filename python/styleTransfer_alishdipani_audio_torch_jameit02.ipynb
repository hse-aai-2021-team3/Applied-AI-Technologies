{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alishdipani_audio_torch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmyqpoXclzT-"
      },
      "source": [
        "Neural Transfer of Audio in Pytorch\n",
        "=========================\n",
        "\n",
        "Neural Style transfer is really interesting. They've been some really interesting applications of style transfer. It basically aims to take the 'style' from one image and change the 'content' image to meet that style. Here's an example. This image has been converted to look like it was painted by Van gough.\n",
        "\n",
        "![photo](https://camo.githubusercontent.com/974884c2fb949b365c3f415b3712d2cac04a35f7/68747470733a2f2f692e696d6775722e636f6d2f575771364931552e6a7067)\n",
        "\n",
        "\n",
        "But so far it hasn't really been applied to audio. So this week I explored the idea of applying neural style transfer to audio. To be frank, the results were less than stellar but I'm hoping to keep working on this in the future. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ngoGFMgsIA"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZzywPCGCXa-"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "from IPython.display import Audio\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGqsFBOVzWwy"
      },
      "source": [
        "# Build dataset\n",
        "\n",
        "For this exercise, I'm going to be using clips from the joe rogan podcast. I'm trying to make [joe rogan](https://en.wikipedia.org/wiki/Joe_Rogan), from the [joe rogan podcast](http://podcasts.joerogan.net/), sound like [joey diaz](https://en.wikipedia.org/wiki/Joey_Diaz), from the [Church of Whats Happening Now](https://www.youtube.com/channel/UCv695o3i-JmkUB7tPbtwXDA). Joe Rogan already does a pretty good [impression of joey diaz](https://www.youtube.com/watch?v=SLolljsbbFs). But I'd like to improve his impression using deep learning.\n",
        "\n",
        "First I'm going to download the youtube videos. There's a neat trick mentioned on github that allows you to download small segments of youtube videos. That's handy cause I don't want to download the entire video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTXTEZ3_Hkn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554a1a78-e50f-4c43-9ec4-10b8bf288544"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJvBUcThK5hE"
      },
      "source": [
        "#content_audio_name = '/content/drive/MyDrive/StyleChanger/content/accompaniment.wav' #change paths\n",
        "#style_audio_name = '/content/drive/MyDrive/StyleChanger/style_synced/accompaniment.wav'\n",
        "\n",
        "content_audio_name = '/content/drive/MyDrive/StyleChanger/content/vocals.wav' #change paths\n",
        "style_audio_name = '/content/drive/MyDrive/StyleChanger/style_synced/vocals.wav'"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzJ1sybvNj-7"
      },
      "source": [
        "# Loss\n",
        "\n",
        "There are two types of loss for this:\n",
        "\n",
        "1. Content loss. Lower values for this means that the output audio sounds like joe rogan. \n",
        "\n",
        "2. Style loss. Lower values for this means that the output audio sounds like joey diaz.\n",
        "\n",
        "Ideally we want both content and style loss to be minimised.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laOvOlsaRgmc"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3y373ZVgs1X"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Conv2d, ReLU, AvgPool1d, MaxPool2d, Linear, Conv1d\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import copy\n",
        "import librosa\n",
        "\n",
        "\n",
        "\n",
        "class GramMatrix(nn.Module):\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\ta, b, c = input.size()  # a=batch size(=1)\n",
        "\t\t\t\t# b=number of feature maps\n",
        "\t\t\t\t# (c,d)=dimensions of a f. map (N=c*d)\n",
        "\t\tfeatures = input.view(a * b, c)  # resise F_XL into \\hat F_XL\n",
        "\t\tG = torch.mm(features, features.t())  # compute the gram product\n",
        "\t\t\t\t# we 'normalize' the values of the gram matrix\n",
        "\t\t\t\t# by dividing by the number of element in each feature maps.\n",
        "\t\treturn G.div(a * b * c)\n",
        "\t\n",
        "\n",
        "# https://ghamrouni.github.io/stn-tuto/advanced/neural_style_tutorial.html#\n",
        "class ContentLoss(nn.Module):\n",
        "\n",
        "\t\tdef __init__(self, target, weight):\n",
        "\t\t\t\tsuper(ContentLoss, self).__init__()\n",
        "\t\t\t\t# we 'detach' the target content from the tree used\n",
        "\t\t\t\tself.target = target.detach() * weight\n",
        "\t\t\t\t# to dynamically compute the gradient: this is a stated value,\n",
        "\t\t\t\t# not a variable. Otherwise the forward method of the criterion\n",
        "\t\t\t\t# will throw an error.\n",
        "\t\t\t\tself.weight = weight\n",
        "\t\t\t\tself.criterion = nn.MSELoss()\n",
        "\n",
        "\t\tdef forward(self, input):\n",
        "\t\t\t\tself.loss = self.criterion(input * self.weight, self.target)\n",
        "\t\t\t\tself.output = input\n",
        "\t\t\t\treturn self.output\n",
        "\n",
        "\t\tdef backward(self, retain_graph=True):\n",
        "\t\t\t\tself.loss.backward(retain_graph=retain_graph)\n",
        "\t\t\t\treturn self.loss\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "\n",
        "\tdef __init__(self, target, weight):\n",
        "\t\tsuper(StyleLoss, self).__init__()\n",
        "\t\tself.target = target.detach() * weight\n",
        "\t\tself.weight = weight\n",
        "\t\tself.gram = GramMatrix()\n",
        "\t\tself.criterion = nn.MSELoss()\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\tself.output = input.clone()\n",
        "\t\tself.G = self.gram(input)\n",
        "\t\tself.G.mul_(self.weight)\n",
        "\t\tself.loss = self.criterion(self.G, self.target)\n",
        "\t\treturn self.output\n",
        "\n",
        "\tdef backward(self,retain_graph=True):\n",
        "\t\tself.loss.backward(retain_graph=retain_graph)\n",
        "\t\treturn self.loss"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABlfWrmJcNQF"
      },
      "source": [
        "# Converting Wav to Matrix\n",
        "\n",
        "To convert the waveform audio to a matrix that we can pass to pytorch I'll use `librosa`. Most of this code was borrowed from Dmitry Ulyanov's [github repo](https://github.com/DmitryUlyanov/neural-style-audio-tf/blob/master/neural-style-audio-tf.ipynb) and Alish Dipani's [github repo](https://github.com/alishdipani/Neural-Style-Transfer-Audio). \n",
        "\n",
        "We get the Short-time Fourier transform from the audio using the `librosa` library. The window size for this is `2048`, which is also the default setting. There is scope here for replacing the code with code from torchaudio. But this works for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FtFef7v7iER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22aa8e23-d4b9-41e1-8a6c-d26ae41a1429"
      },
      "source": [
        "import gc; gc.collect()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Yh70aGP6-6"
      },
      "source": [
        "# USING LIBROSA\n",
        "N_FFT=2048\n",
        "def read_audio_spectum(filename):\n",
        "  x, fs = librosa.load(filename)\n",
        "  S = librosa.stft(x, N_FFT)\n",
        "  p = np.angle(S)\n",
        "  S = np.log1p(np.abs(S))  \n",
        "  return S, fs\n",
        "\n",
        "style_audio, style_sr = read_audio_spectum(style_audio_name)\n",
        "content_audio, content_sr = read_audio_spectum(content_audio_name)\n",
        "\n",
        "if(content_sr != style_sr):\n",
        "  raise 'Sampling rates are not same'\n",
        "\n",
        "  \n",
        "style_audio = style_audio.reshape([1,1025,style_audio.shape[1]])\n",
        "content_audio = content_audio.reshape([1,1025,content_audio.shape[1]])\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  style_float = Variable((torch.from_numpy(style_audio)).cuda())\n",
        "  content_float = Variable((torch.from_numpy(content_audio)).cuda())\t\n",
        "else:\n",
        "  style_float = Variable(torch.from_numpy(style_audio))\n",
        "  content_float = Variable(torch.from_numpy(content_audio))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc0_sxe0Ustu"
      },
      "source": [
        "# !pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFGBnAZUqr_W"
      },
      "source": [
        "# Create CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCu9bCbE1Kad"
      },
      "source": [
        "import gc; gc.collect(); del cnn"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2iadNf018iE"
      },
      "source": [
        "This CNN is very shallow. It consists of 2 convolutions and a ReLU in between them. I originally took the CNN used [here](https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py) but I've made a few changes. \n",
        "\n",
        " - Firstly, I added content loss. This wasn't added before and is obviously very useful. We'd like to know how close (or far away) the audio sounds to the original content.\n",
        "\n",
        " - Secondly, I added a ReLU to the model. It's pretty well [established](https://stats.stackexchange.com/questions/275358/why-is-increasing-the-non-linearity-of-neural-networks-desired) that nonlinear activations are desired in a neural network. Adding a ReLU improved the model significantly.\n",
        "\n",
        " - Increased the number of steps. From ``2500`` to `20000`\n",
        "\n",
        " - Slightly deepened the network. I added a layer of `Conv1d`. After this layer style loss and content loss is calculated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGPubl9D3zrr"
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(CNNModel, self).__init__()\n",
        "\t\tself.cnn1 = Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "\t\tself.relu = ReLU()\n",
        "\t\tself.cnn2 = Conv1d(in_channels=4096, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = self.cnn1(x)\n",
        "\t\tout = self.relu(out)\n",
        "\t\tout = self.cnn2(x)\n",
        "\t\tout = self.relu(out)\n",
        "\t\tout = self.cnn3(x)\n",
        "\t\treturn out"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7RqNYVhCQ3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e859fb-1321-48ff-dedd-376d8420a82e"
      },
      "source": [
        "cnn = CNNModel()\n",
        "if torch.cuda.is_available():\n",
        "  cnn = cnn.cuda()\n",
        "\n",
        "\n",
        "style_weight=1000\n",
        "content_weight = 2\n",
        "\n",
        "\n",
        "def get_style_model_and_losses(cnn, style_float,\\\n",
        "                               content_float=content_float,\\\n",
        "                               style_weight=style_weight):\n",
        "  \n",
        "  cnn = copy.deepcopy(cnn)\n",
        "\n",
        "  style_losses = []\n",
        "  content_losses = []\n",
        "\n",
        "  # create model\n",
        "  model = nn.Sequential()\n",
        "\n",
        "  # we need a gram module in order to compute style targets\n",
        "  gram = GramMatrix()\n",
        "\n",
        "  # load onto gpu  \n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    gram = gram.cuda()\n",
        "\n",
        "  # add conv1\n",
        "  name = 'conv_1'\n",
        "  model.add_module(name, cnn.cnn1)\n",
        "\n",
        "  # add relu\n",
        "  name = 'relu1'\n",
        "  model.add_module(name, cnn.relu)\n",
        "\n",
        "  # add conv2\n",
        "  name = 'conv_2'\n",
        "  model.add_module(name, cnn.cnn2)\n",
        "\n",
        "  # add style loss\n",
        "  target_feature = model(style_float).clone()\n",
        "  target_feature_gram = gram(target_feature)\n",
        "  style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "  model.add_module(\"style_loss_1\", style_loss)\n",
        "  style_losses.append(style_loss)\n",
        "\n",
        "  # add content loss\n",
        "  target = model(content_float).detach()\n",
        "  content_loss = ContentLoss(target, content_weight)\n",
        "  model.add_module(\"content_loss_1\", content_loss)\n",
        "  content_losses.append(content_loss)\n",
        "\n",
        "  return model, style_losses, content_losses\n",
        "\n",
        "\n",
        "get_style_model_and_losses(cnn, style_float, content_float)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Sequential(\n",
              "   (conv_1): Conv1d(1025, 4096, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "   (relu1): ReLU()\n",
              "   (conv_2): Conv1d(4096, 4096, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "   (style_loss_1): StyleLoss(\n",
              "     (gram): GramMatrix()\n",
              "     (criterion): MSELoss()\n",
              "   )\n",
              "   (content_loss_1): ContentLoss(\n",
              "     (criterion): MSELoss()\n",
              "   )\n",
              " ), [StyleLoss(\n",
              "    (gram): GramMatrix()\n",
              "    (criterion): MSELoss()\n",
              "  )], [ContentLoss(\n",
              "    (criterion): MSELoss()\n",
              "  )])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGPgaB-1fie8"
      },
      "source": [
        "# Run style transfer\n",
        "\n",
        "Now I'll run the style transfer. This will use the `optim.Adam` optimizer. This piece of code was taken from the pytorch tutorial for [neural style transfer](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html). For each iteration of the network the style loss and content loss is calculated. In turn that is used to get the gradients. The gradients are mulitplied by the learnign rates. That in turn updates the input audio matrix. In pytorch the optimizer requries a [closure](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html#gradient-descent) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "908CdwL0wXjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b637bea-67de-4e64-f3b1-ed1e48ffc823"
      },
      "source": [
        "import gc; gc.collect()\n",
        "\n",
        "input_float = content_float.clone()\n",
        "#input_float = Variable(torch.randn(content_float.size())).type(torch.FloatTensor)\n",
        "\n",
        "learning_rate_initial = 1e-4\n",
        "\n",
        "def get_input_param_optimizer(input_float):\n",
        "  input_param = nn.Parameter(input_float.data)\n",
        "  # optimizer = optim.Adagrad([input_param], lr=learning_rate_initial, lr_decay=0.0001,weight_decay=0)\n",
        "  optimizer = optim.Adam([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.LBFGS([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.SGD([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.RMSprop([input_param], lr=learning_rate_initial)\n",
        "  return input_param, optimizer\n",
        "\n",
        "num_steps= 1000\n",
        "\n",
        "\n",
        "# from https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
        "def run_style_transfer(cnn, style_float=style_float,\\\n",
        "                       content_float=content_float,\\\n",
        "                       input_float=input_float,\\\n",
        "                       num_steps=num_steps, style_weight=style_weight): \n",
        "  print('Building the style transfer model..')\n",
        "  # model, style_losses = get_style_model_and_losses(cnn, style_float)\n",
        "  model, style_losses, content_losses = get_style_model_and_losses(cnn, style_float, content_float)\n",
        "  input_param, optimizer = get_input_param_optimizer(input_float)\n",
        "  print('Optimizing..')\n",
        "  run = [0]\n",
        "\n",
        "  while run[0] <= num_steps:\n",
        "    def closure():\n",
        "            # correct the values of updated input image\n",
        "      input_param.data.clamp_(0, 1)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      model(input_param)\n",
        "      style_score = 0\n",
        "      content_score = 0\n",
        "\n",
        "      for sl in style_losses:\n",
        "        #print('sl is ',sl,' style loss is ',style_score)\n",
        "        style_score += sl.loss\n",
        "\n",
        "      for cl in content_losses:\n",
        "        content_score += cl.loss\n",
        "\n",
        "      style_score *= style_weight\n",
        "      content_score *= content_weight\n",
        "\n",
        "      loss = style_score + content_score\n",
        "      loss.backward()\n",
        "\n",
        "      run[0] += 1\n",
        "      if run[0] % 100 == 0:\n",
        "        print(\"run {}:\".format(run))\n",
        "        print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
        "                    style_score.item(), content_score.item()))\n",
        "        print()\n",
        "\n",
        "      return style_score + content_score\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "  # ensure values are between 0 and 1\n",
        "  input_param.data.clamp_(0, 1)\n",
        "\n",
        "  return input_param.data\n",
        "\n",
        "\n",
        "output = run_style_transfer(cnn, style_float=style_float, content_float=content_float, input_float=input_float)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the style transfer model..\n",
            "Optimizing..\n",
            "run [100]:\n",
            "Style Loss : 0.012030 Content Loss: 0.014665\n",
            "\n",
            "run [200]:\n",
            "Style Loss : 0.011779 Content Loss: 0.014061\n",
            "\n",
            "run [300]:\n",
            "Style Loss : 0.011545 Content Loss: 0.013543\n",
            "\n",
            "run [400]:\n",
            "Style Loss : 0.011324 Content Loss: 0.013094\n",
            "\n",
            "run [500]:\n",
            "Style Loss : 0.011116 Content Loss: 0.012702\n",
            "\n",
            "run [600]:\n",
            "Style Loss : 0.010921 Content Loss: 0.012357\n",
            "\n",
            "run [700]:\n",
            "Style Loss : 0.010737 Content Loss: 0.012052\n",
            "\n",
            "run [800]:\n",
            "Style Loss : 0.010564 Content Loss: 0.011780\n",
            "\n",
            "run [900]:\n",
            "Style Loss : 0.010400 Content Loss: 0.011538\n",
            "\n",
            "run [1000]:\n",
            "Style Loss : 0.010245 Content Loss: 0.011320\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmlYYpFSq9Tr"
      },
      "source": [
        "# Reconstruct Audio\n",
        "\n",
        "Finally the audio needs to be reconstructed. To do that the librosa inverse short-time fourier transform can be used. \n",
        "\n",
        "Then we write to an audio file and use the jupyter notebook extension to play the audio in the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5rtCog9cjqC"
      },
      "source": [
        "# taken from: https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  output = output.cpu()\n",
        "\n",
        "output = output.squeeze(0)\n",
        "output = output.numpy()\n",
        "\n",
        "N_FFT=2048\n",
        "a = np.zeros_like(output)\n",
        "a = np.exp(output) - 1\n",
        "\n",
        "# This code is supposed to do phase reconstruction\n",
        "p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "for i in range(500):\n",
        "  S = a * np.exp(1j*p)\n",
        "  x = librosa.istft(S)\n",
        "  p = np.angle(librosa.stft(x, N_FFT))\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d87lO8OiTRG_"
      },
      "source": [
        "import soundfile as sf\n",
        "\n",
        "OUTPUT_FILENAME = '/content/drive/MyDrive/StyleChanger/testV4.wav'\n",
        "sf.write(OUTPUT_FILENAME, x, style_sr)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHkmVl0wTTYf"
      },
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}