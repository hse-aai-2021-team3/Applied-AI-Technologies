{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getting_started.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "P_NeO0dKppjH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sehait02/Applied-AI-Technologies/blob/StyleChangerCleanup/python/Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyrQAkkrouNT"
      },
      "source": [
        "pip install audiotsm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD1aF1nBpwYJ"
      },
      "source": [
        "!pip install ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqUDJ6SSqDbD"
      },
      "source": [
        "pip install spleeter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2wvtO9k5iZG"
      },
      "source": [
        "from google.colab import files\n",
        "#from IPython.display import Audio #for testing\n",
        "import os\n",
        "import shutil\n",
        "# Imports\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "import librosa\n",
        "from librosa import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import math\n",
        "import pywt\n",
        "from scipy import signal\n",
        "\n",
        "from audiotsm import phasevocoder\n",
        "from audiotsm.io.wav import WavReader, WavWriter\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L33x8PHJqXDi"
      },
      "source": [
        "from IPython.display import Audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLZFaAGqssY4"
      },
      "source": [
        "import torch\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PXMjaVIu581"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import Conv2d, ReLU, AvgPool1d, MaxPool2d, Linear, Conv1d\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Rsonz0ps8F"
      },
      "source": [
        "# Upload"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt0wVKDS68Qv"
      },
      "source": [
        "#upload style file and rename to style.wav\n",
        "a = 1\n",
        "while a:\n",
        "  style = files.upload()\n",
        "  for fn in style.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(style[fn])))\n",
        "  if len(style.keys()) > 1:\n",
        "    print('UPLOAD ONLY ONE FILE')\n",
        "  else:\n",
        "    stylename = fn\n",
        "    a = 0\n",
        "  del style"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn5Ofo0MD73e"
      },
      "source": [
        "#upload content file and rename to content.wav\n",
        "b = 1\n",
        "while b:\n",
        "  content = files.upload()\n",
        "  for fnb in content.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fnb, length=len(content[fnb])))\n",
        "  if len(content.keys()) > 1:\n",
        "    print('UPLOAD ONLY ONE FILE')\n",
        "  else:\n",
        "    contentname = fnb\n",
        "    b = 0\n",
        "  del content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkfevPb5Kk47"
      },
      "source": [
        "try:\n",
        "    os.remove(\"style.wav\")\n",
        "    os.remove(\"content.wav\")\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "shutil.move(stylename, \"style.wav\")\n",
        "shutil.move(contentname, \"content.wav\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGAy-00T8_xl"
      },
      "source": [
        "you can add an 'Audio(filemane)' at any point to listen to valid audio files:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iwcdo6V9FRq"
      },
      "source": [
        "#Audio('style.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_NeO0dKppjH"
      },
      "source": [
        "# Sync BPM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIS_MjeNO5bd"
      },
      "source": [
        "def read_audio_spectum_extended(filename):\n",
        "    x, fs = librosa.load(filename)\n",
        "    S = librosa.stft(x, N_FFT)\n",
        "    S = np.log1p(np.abs(S)) \n",
        "    return S, x, fs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-PsFSndo4Mb"
      },
      "source": [
        "# SOURCE: https://github.com/scaperot/the-BPM-detector-python/blob/master/bpm_detection/bpm_detection.py\n",
        "def no_audio_data():\n",
        "    print(\"No audio data for sample, skipping...\")\n",
        "    return None, None\n",
        "\n",
        "def peak_detect(data):\n",
        "    max_val = np.amax(abs(data))\n",
        "    peak_ndx = np.where(data == max_val)\n",
        "    if len(peak_ndx[0]) == 0:  # if nothing found then the max must be negative\n",
        "        peak_ndx = np.where(data == -max_val)\n",
        "    return peak_ndx\n",
        "\n",
        "def bpm_detector(data, fs):\n",
        "    cA = []\n",
        "    cD = []\n",
        "    correl = []\n",
        "    cD_sum = []\n",
        "    levels = 4\n",
        "    max_decimation = 2 ** (levels - 1)\n",
        "    min_ndx = math.floor(60.0 / 220 * (fs / max_decimation))\n",
        "    max_ndx = math.floor(60.0 / 40 * (fs / max_decimation))\n",
        "\n",
        "    for loop in range(0, levels):\n",
        "        cD = []\n",
        "        # 1) DWT\n",
        "        if loop == 0:\n",
        "            [cA, cD] = pywt.dwt(data, \"db4\")\n",
        "            cD_minlen = len(cD) / max_decimation + 1\n",
        "            cD_sum = np.zeros(math.floor(cD_minlen))\n",
        "        else:\n",
        "            [cA, cD] = pywt.dwt(cA, \"db4\")\n",
        "\n",
        "        # 2) Filter\n",
        "        cD = signal.lfilter([0.01], [1 - 0.99], cD)\n",
        "\n",
        "        # 4) Subtract out the mean.\n",
        "\n",
        "        # 5) Decimate for reconstruction later.\n",
        "        cD = abs(cD[:: (2 ** (levels - loop - 1))])\n",
        "        cD = cD - np.mean(cD)\n",
        "\n",
        "        # 6) Recombine the signal before ACF\n",
        "        #    Essentially, each level the detail coefs (i.e. the HPF values) are concatenated to the beginning of the array\n",
        "        cD_sum = cD[0 : math.floor(cD_minlen)] + cD_sum\n",
        "\n",
        "    if [b for b in cA if b != 0.0] == []:\n",
        "        return no_audio_data()\n",
        "\n",
        "    # Adding in the approximate data as well...\n",
        "    cA = signal.lfilter([0.01], [1 - 0.99], cA)\n",
        "    cA = abs(cA)\n",
        "    cA = cA - np.mean(cA)\n",
        "    cD_sum = cA[0 : math.floor(cD_minlen)] + cD_sum\n",
        "\n",
        "    # ACF\n",
        "    correl = np.correlate(cD_sum, cD_sum, \"full\")\n",
        "\n",
        "    midpoint = math.floor(len(correl) / 2)\n",
        "    correl_midpoint_tmp = correl[midpoint:]\n",
        "    peak_ndx = peak_detect(correl_midpoint_tmp[min_ndx:max_ndx])\n",
        "    if len(peak_ndx) > 1:\n",
        "        return no_audio_data()\n",
        "\n",
        "    peak_ndx_adjusted = peak_ndx[0] + min_ndx\n",
        "    bpm = 60.0 / peak_ndx_adjusted * (fs / max_decimation)\n",
        "    #print(bpm)\n",
        "    return bpm, correl\n",
        "\n",
        "def get_bpm(raw_samples, fs):\n",
        "    data = []\n",
        "    correl = []\n",
        "    bpm = 0\n",
        "    n = 0\n",
        "    nsamps = len(raw_samples)\n",
        "    window_samps = int(window * fs)\n",
        "    samps_ndx = 0  # First sample in window_ndx\n",
        "    max_window_ndx = math.floor(nsamps / window_samps)\n",
        "    bpms = np.zeros(max_window_ndx)\n",
        "    \n",
        "    for window_ndx in range(0, max_window_ndx):\n",
        "\n",
        "        # Get a new set of samples\n",
        "        # print(n,\":\",len(bpms),\":\",max_window_ndx_int,\":\",fs,\":\",nsamps,\":\",samps_ndx)\n",
        "        data = raw_samples[samps_ndx : samps_ndx + window_samps]\n",
        "        if not ((len(data) % window_samps) == 0):\n",
        "            raise AssertionError(str(len(data)))\n",
        "\n",
        "        bpm, correl_temp = bpm_detector(data, fs)\n",
        "        if bpm is None:\n",
        "            continue\n",
        "        bpms[window_ndx] = bpm\n",
        "        correl = correl_temp\n",
        "\n",
        "        # Iterate at the end of the loop\n",
        "        samps_ndx = samps_ndx + window_samps\n",
        "\n",
        "        # Counter for debug...\n",
        "        n = n + 1\n",
        "\n",
        "    bpm = np.median(bpms)\n",
        "    \n",
        "    return bpm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXvx8Fqwo7RN"
      },
      "source": [
        "CONTENT_FILENAME = 'content.wav'\n",
        "STYLE_FILENAME = 'style.wav'\n",
        "STYLE_BPM_MOD_FILENAME = 'style_synced.wav'\n",
        "\n",
        "N_FFT = 2048\n",
        "\n",
        "window = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DEM4-e9pBm-"
      },
      "source": [
        "a_content, raw_content, fs_content = read_audio_spectum_extended(CONTENT_FILENAME)\n",
        "a_style, raw_style, fs_style = read_audio_spectum_extended(STYLE_FILENAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ts3nrSbpD0n"
      },
      "source": [
        "bpm_content = get_bpm(raw_content, fs_content);\n",
        "bpm_style = get_bpm(raw_style, fs_style);\n",
        "print(bpm_content)\n",
        "print(bpm_style)\n",
        "\n",
        "bpm_ratio = bpm_content / bpm_style;\n",
        "print(bpm_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weAJo0BbpF2z"
      },
      "source": [
        "with WavReader(STYLE_FILENAME) as reader:\n",
        "    with WavWriter(STYLE_BPM_MOD_FILENAME, reader.channels, reader.samplerate) as writer:\n",
        "        tsm = phasevocoder(reader.channels, speed=bpm_ratio)\n",
        "        tsm.run(reader, writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFPX9oG3rnzM"
      },
      "source": [
        "# Split audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os-C_q28qxNr"
      },
      "source": [
        "!spleeter separate -o /split style_synced.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z2eGTGsq3cr"
      },
      "source": [
        "!spleeter separate -o /split content.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUHDE0vQs457"
      },
      "source": [
        "# Style transfer via spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWtm1hl9tFJq"
      },
      "source": [
        "#the following code and text is largely taken from:\n",
        "#https://github.com/spiyer99/spiyer99.github.io/blob/master/nbs/Neural%20Transfer%20of%20Audio%20in%20Pytorch.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7PEnEox1aZd"
      },
      "source": [
        "content_audio_name = '/split/content/accompaniment.wav'\n",
        "style_audio_name = '/split/style_synced/accompaniment.wav'\n",
        "content_audio_name2 = '/split/content/vocals.wav'\n",
        "style_audio_name2 = '/split/style_synced/vocals.wav'\n",
        "\n",
        "OUTPUT_FILENAME1 = 'playback.wav'\n",
        "OUTPUT_FILENAME2 = 'vocal.wav'\n",
        "OUTPUT_FILENAME = OUTPUT_FILENAME1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM32ffmduxHo"
      },
      "source": [
        "## Loss\n",
        "\n",
        "There are two types of loss for this:\n",
        "\n",
        "1. Content loss. Lower values for this means that the output audio sounds like joe rogan. \n",
        "\n",
        "2. Style loss. Lower values for this means that the output audio sounds like joey diaz.\n",
        "\n",
        "Ideally we want both content and style loss to be minimised.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mERpcn8uuzo7"
      },
      "source": [
        "class GramMatrix(nn.Module):\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\ta, b, c = input.size()  # a=batch size(=1)\n",
        "\t\t\t\t# b=number of feature maps\n",
        "\t\t\t\t# (c,d)=dimensions of a f. map (N=c*d)\n",
        "\t\tfeatures = input.view(a * b, c)  # resise F_XL into \\hat F_XL\n",
        "\t\tG = torch.mm(features, features.t())  # compute the gram product\n",
        "\t\t\t\t# we 'normalize' the values of the gram matrix\n",
        "\t\t\t\t# by dividing by the number of element in each feature maps.\n",
        "\t\treturn G.div(a * b * c)\n",
        "\t\n",
        "\n",
        "# https://ghamrouni.github.io/stn-tuto/advanced/neural_style_tutorial.html#\n",
        "class ContentLoss(nn.Module):\n",
        "\n",
        "\t\tdef __init__(self, target, weight):\n",
        "\t\t\t\tsuper(ContentLoss, self).__init__()\n",
        "\t\t\t\t# we 'detach' the target content from the tree used\n",
        "\t\t\t\tself.target = target.detach() * weight\n",
        "\t\t\t\t# to dynamically compute the gradient: this is a stated value,\n",
        "\t\t\t\t# not a variable. Otherwise the forward method of the criterion\n",
        "\t\t\t\t# will throw an error.\n",
        "\t\t\t\tself.weight = weight\n",
        "\t\t\t\tself.criterion = nn.MSELoss()\n",
        "\n",
        "\t\tdef forward(self, input):\n",
        "\t\t\t\tself.loss = self.criterion(input * self.weight, self.target)\n",
        "\t\t\t\tself.output = input\n",
        "\t\t\t\treturn self.output\n",
        "\n",
        "\t\tdef backward(self, retain_graph=True):\n",
        "\t\t\t\tself.loss.backward(retain_graph=retain_graph)\n",
        "\t\t\t\treturn self.loss\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "\n",
        "\tdef __init__(self, target, weight):\n",
        "\t\tsuper(StyleLoss, self).__init__()\n",
        "\t\tself.target = target.detach() * weight\n",
        "\t\tself.weight = weight\n",
        "\t\tself.gram = GramMatrix()\n",
        "\t\tself.criterion = nn.MSELoss()\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\tself.output = input.clone()\n",
        "\t\tself.G = self.gram(input)\n",
        "\t\tself.G.mul_(self.weight)\n",
        "\t\tself.loss = self.criterion(self.G, self.target)\n",
        "\t\treturn self.output\n",
        "\n",
        "\tdef backward(self,retain_graph=True):\n",
        "\t\tself.loss.backward(retain_graph=retain_graph)\n",
        "\t\treturn self.loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N10_FpLJvJJy"
      },
      "source": [
        "## Converting Wav to Matrix\n",
        "\n",
        "To convert the waveform audio to a matrix that we can pass to pytorch I'll use `librosa`. Most of this code was borrowed from Dmitry Ulyanov's [github repo](https://github.com/DmitryUlyanov/neural-style-audio-tf/blob/master/neural-style-audio-tf.ipynb) and Alish Dipani's [github repo](https://github.com/alishdipani/Neural-Style-Transfer-Audio). \n",
        "\n",
        "We get the Short-time Fourier transform from the audio using the `librosa` library. The window size for this is `2048`, which is also the default setting. There is scope here for replacing the code with code from torchaudio. But this works for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmuskbdZvQpk"
      },
      "source": [
        "import gc; gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waAKXDStvTT4"
      },
      "source": [
        "# USING LIBROSA\n",
        "N_FFT=2048\n",
        "def read_audio_spectum(filename):\n",
        "  x, fs = librosa.load(filename)\n",
        "  S = librosa.stft(x, N_FFT)\n",
        "  p = np.angle(S)\n",
        "  S = np.log1p(np.abs(S))  \n",
        "  return S, fs\n",
        "\n",
        "style_audio, style_sr = read_audio_spectum(style_audio_name)\n",
        "content_audio, content_sr = read_audio_spectum(content_audio_name)\n",
        "\n",
        "style_audio2, style_sr2 = read_audio_spectum(style_audio_name2)\n",
        "content_audio2, content_sr2 = read_audio_spectum(content_audio_name2)\n",
        "\n",
        "if(content_sr != style_sr):\n",
        "  raise 'Sampling rates are not same'\n",
        "\n",
        "if(content_sr2 != style_sr2):\n",
        "  raise 'Sampling rates are not same'\n",
        "\n",
        "  \n",
        "style_audio = style_audio.reshape([1,1025,style_audio.shape[1]])\n",
        "content_audio = content_audio.reshape([1,1025,content_audio.shape[1]])\n",
        "\n",
        "style_audio2 = style_audio2.reshape([1,1025,style_audio2.shape[1]])\n",
        "content_audio2 = content_audio2.reshape([1,1025,content_audio2.shape[1]])\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  style_float = Variable((torch.from_numpy(style_audio)).cuda())\n",
        "  content_float = Variable((torch.from_numpy(content_audio)).cuda())\t\n",
        "\n",
        "  style_float2 = Variable((torch.from_numpy(style_audio2)).cuda())\n",
        "  content_float2 = Variable((torch.from_numpy(content_audio2)).cuda())\n",
        "else:\n",
        "  style_float = Variable(torch.from_numpy(style_audio))\n",
        "  content_float = Variable(torch.from_numpy(content_audio))\n",
        "\n",
        "  style_float2 = Variable(torch.from_numpy(style_audio2))\n",
        "  content_float2 = Variable(torch.from_numpy(content_audio2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Lwbgo5vZfj"
      },
      "source": [
        "## Create CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsS67DLh7qFC"
      },
      "source": [
        "here is the start of the part which has to be run twice\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVQULEVIvbVT"
      },
      "source": [
        "import gc;\n",
        "gc.collect();\n",
        "if 'cnn' in locals():\n",
        "  del cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppq0vzZjvfhi"
      },
      "source": [
        "This CNN is very shallow. It consists of 2 convolutions and a ReLU in between them. I originally took the CNN used [here](https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py) but I've made a few changes. \n",
        "\n",
        " - Firstly, I added content loss. This wasn't added before and is obviously very useful. We'd like to know how close (or far away) the audio sounds to the original content.\n",
        "\n",
        " - Secondly, I added a ReLU to the model. It's pretty well [established](https://stats.stackexchange.com/questions/275358/why-is-increasing-the-non-linearity-of-neural-networks-desired) that nonlinear activations are desired in a neural network. Adding a ReLU improved the model significantly.\n",
        "\n",
        " - Increased the number of steps. From ``2500`` to `20000`\n",
        "\n",
        " - Slightly deepened the network. I added a layer of `Conv1d`. After this layer style loss and content loss is calculated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZBDK2YEve_b"
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(CNNModel, self).__init__()\n",
        "\t\tself.cnn1 = Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "\t\tself.relu = ReLU()\n",
        "\t\tself.cnn2 = Conv1d(in_channels=4096, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = self.cnn1(x)\n",
        "\t\tout = self.relu(out)\n",
        "\t\tout = self.cnn2(x)\n",
        "\t\tout = self.relu(out)\n",
        "\t\tout = self.cnn3(x)\n",
        "\t\treturn out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcuQ-1v6wPzE"
      },
      "source": [
        "cnn = CNNModel()\n",
        "if torch.cuda.is_available():\n",
        "  cnn = cnn.cuda()\n",
        "\n",
        "\n",
        "style_weight=1000\n",
        "content_weight = 2\n",
        "\n",
        "\n",
        "def get_style_model_and_losses(cnn, style_float,\\\n",
        "                               content_float=content_float,\\\n",
        "                               style_weight=style_weight):\n",
        "  \n",
        "  cnn = copy.deepcopy(cnn)\n",
        "\n",
        "  style_losses = []\n",
        "  content_losses = []\n",
        "\n",
        "  # create model\n",
        "  model = nn.Sequential()\n",
        "\n",
        "  # we need a gram module in order to compute style targets\n",
        "  gram = GramMatrix()\n",
        "\n",
        "  # load onto gpu  \n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    gram = gram.cuda()\n",
        "\n",
        "  # add conv1\n",
        "  name = 'conv_1'\n",
        "  model.add_module(name, cnn.cnn1)\n",
        "\n",
        "  # add relu\n",
        "  name = 'relu1'\n",
        "  model.add_module(name, cnn.relu)\n",
        "\n",
        "  # add conv2\n",
        "  name = 'conv_2'\n",
        "  model.add_module(name, cnn.cnn2)\n",
        "\n",
        "  # add style loss\n",
        "  target_feature = model(style_float).clone()\n",
        "  target_feature_gram = gram(target_feature)\n",
        "  style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "  model.add_module(\"style_loss_1\", style_loss)\n",
        "  style_losses.append(style_loss)\n",
        "\n",
        "  # add content loss\n",
        "  target = model(content_float).detach()\n",
        "  content_loss = ContentLoss(target, content_weight)\n",
        "  model.add_module(\"content_loss_1\", content_loss)\n",
        "  content_losses.append(content_loss)\n",
        "\n",
        "  return model, style_losses, content_losses\n",
        "\n",
        "\n",
        "get_style_model_and_losses(cnn, style_float, content_float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22T7zGpGwTOv"
      },
      "source": [
        "import gc; gc.collect()\n",
        "\n",
        "input_float = content_float.clone()\n",
        "#input_float = Variable(torch.randn(content_float.size())).type(torch.FloatTensor)\n",
        "\n",
        "learning_rate_initial = 1e-4\n",
        "\n",
        "def get_input_param_optimizer(input_float):\n",
        "  input_param = nn.Parameter(input_float.data)\n",
        "  # optimizer = optim.Adagrad([input_param], lr=learning_rate_initial, lr_decay=0.0001,weight_decay=0)\n",
        "  optimizer = optim.Adam([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.LBFGS([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.SGD([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.RMSprop([input_param], lr=learning_rate_initial)\n",
        "  return input_param, optimizer\n",
        "\n",
        "num_steps= 1000 #number of steps\n",
        "\n",
        "\n",
        "# from https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
        "def run_style_transfer(cnn, style_float=style_float,\\\n",
        "                       content_float=content_float,\\\n",
        "                       input_float=input_float,\\\n",
        "                       num_steps=num_steps, style_weight=style_weight): \n",
        "  print('Building the style transfer model..')\n",
        "  # model, style_losses = get_style_model_and_losses(cnn, style_float)\n",
        "  model, style_losses, content_losses = get_style_model_and_losses(cnn, style_float, content_float)\n",
        "  input_param, optimizer = get_input_param_optimizer(input_float)\n",
        "  print('Optimizing..')\n",
        "  run = [0]\n",
        "\n",
        "  while run[0] <= num_steps:\n",
        "    def closure():\n",
        "            # correct the values of updated input image\n",
        "      input_param.data.clamp_(0, 1)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      model(input_param)\n",
        "      style_score = 0\n",
        "      content_score = 0\n",
        "\n",
        "      for sl in style_losses:\n",
        "        #print('sl is ',sl,' style loss is ',style_score)\n",
        "        style_score += sl.loss\n",
        "\n",
        "      for cl in content_losses:\n",
        "        content_score += cl.loss\n",
        "\n",
        "      style_score *= style_weight\n",
        "      content_score *= content_weight\n",
        "\n",
        "      loss = style_score + content_score\n",
        "      loss.backward()\n",
        "\n",
        "      run[0] += 1\n",
        "      if run[0] % 100 == 0:\n",
        "        print(\"run {}:\".format(run))\n",
        "        print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
        "                    style_score.item(), content_score.item()))\n",
        "        print()\n",
        "\n",
        "      return style_score + content_score\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "  # ensure values are between 0 and 1\n",
        "  input_param.data.clamp_(0, 1)\n",
        "\n",
        "  return input_param.data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0upU5bL_w2EU"
      },
      "source": [
        "output = run_style_transfer(cnn, style_float=style_float, content_float=content_float, input_float=input_float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlcc8JurxBk_"
      },
      "source": [
        "## Reconstruct Audio\n",
        "\n",
        "Finally the audio needs to be reconstructed. To do that the librosa inverse short-time fourier transform can be used. \n",
        "\n",
        "Then we write to an audio file and use the jupyter notebook extension to play the audio in the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtbAbYjqxC_x"
      },
      "source": [
        "# taken from: https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  output = output.cpu()\n",
        "\n",
        "output = output.squeeze(0)\n",
        "output = output.numpy()\n",
        "\n",
        "N_FFT=2048\n",
        "a = np.zeros_like(output)\n",
        "a = np.exp(output) - 1\n",
        "\n",
        "# This code is supposed to do phase reconstruction\n",
        "p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "for i in range(500):\n",
        "  S = a * np.exp(1j*p)\n",
        "  x = librosa.istft(S)\n",
        "  p = np.angle(librosa.stft(x, N_FFT))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26j6spbExJHG"
      },
      "source": [
        "import soundfile as sf\n",
        "sf.write(OUTPUT_FILENAME, x, style_sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFGnc5Mt7U4P"
      },
      "source": [
        "if OUTPUT_FILENAME == OUTPUT_FILENAME2:\n",
        "  wavCnt = 2\n",
        "else:\n",
        "  wavCnt = 1\n",
        "  style_float = style_float2\n",
        "  content_float = content_float2\n",
        "  OUTPUT_FILENAME = OUTPUT_FILENAME2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbHj2DX78WVC"
      },
      "source": [
        "# Combine results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odU1UkqF9zSU"
      },
      "source": [
        "if wavCnt == 2:\n",
        "  samples1, fs1 = librosa.load(OUTPUT_FILENAME1)\n",
        "  samples2, fs2 = librosa.load(OUTPUT_FILENAME2)\n",
        "\n",
        "  samples = 0.5 * samples1\n",
        "  samples += 0.5 * samples2\n",
        "\n",
        "  fs = fs1\n",
        "\n",
        "  # Save output data\n",
        "  import soundfile as sf\n",
        "  sf.write(\"transfered_sum.wav\", samples, fs)\n",
        "\n",
        "  DEMO_FILENAME = \"transfered_sum.wav\"\n",
        "  print(\"Done\")\n",
        "else:\n",
        "  DEMO_FILENAME = OUTPUT_FILENAME1\n",
        "  print(\"Now you have to run everything from box 25 again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGHiCFY83vzL"
      },
      "source": [
        "Audio(DEMO_FILENAME)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}